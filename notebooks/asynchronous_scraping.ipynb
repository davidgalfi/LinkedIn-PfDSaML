{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import aiohttp for making asynchronous HTTP requests\n",
    "import aiohttp\n",
    "\n",
    "# Import asyncio for writing asynchronous code using coroutines\n",
    "import asyncio\n",
    "\n",
    "# Import nest_asyncio to allow nested use of asyncio.run() in Jupyter notebooks\n",
    "import nest_asyncio\n",
    "\n",
    "# Import BeautifulSoup from the bs4 library for parsing HTML and XML documents\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Import the csv module for reading and writing CSV files\n",
    "import csv\n",
    "\n",
    "# Import the re module for performing regular expression operations\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the nest_asyncio patch to allow nested use of asyncio event loops\n",
    "# This is particularly useful in Jupyter notebooks where the event loop is already running\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an asynchronous function to scrape links and save them to a CSV file\n",
    "async def scrap_and_save_links(text):\n",
    "    # Parse the HTML content using BeautifulSoup with 'html.parser'\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    \n",
    "    # Open a CSV file named 'csv_file.csv' in append mode to store the scraped links\n",
    "    # Use 'newline=\"\"' to avoid adding extra newlines on Windows\n",
    "    file = open('csv_file.csv', 'a', newline='')\n",
    "    \n",
    "    # Create a CSV writer object with a comma as the delimiter\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    \n",
    "    # Iterate over all <a> tags with 'href' attributes starting with 'http'\n",
    "    for link in soup.find_all('a', attrs={'href': re.compile(\"^http\")}):\n",
    "        # Get the 'href' attribute value of each link\n",
    "        link = link.get('href')\n",
    "        \n",
    "        # Write the link to the CSV file as a new row\n",
    "        writer.writerow([link])\n",
    "    \n",
    "    # Close the file after writing all links\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an asynchronous function to fetch an URL and process its content\n",
    "async def fetch(session, url):\n",
    "    try:\n",
    "        # Use the aiohttp session to make an asynchronous GET request to the specified URL\n",
    "        async with session.get(url) as response:\n",
    "            # Await the response text, which contains the HTML content of the page\n",
    "            text = await response.text()\n",
    "            \n",
    "            # Create an asynchronous task to scrape links from the HTML content and save them\n",
    "            task = asyncio.create_task(scrap_and_save_links(text))\n",
    "            \n",
    "            # Await the completion of the scraping and saving task\n",
    "            await task\n",
    "    except Exception as e:\n",
    "        # Print any exceptions that occur during the fetching or processing\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an asynchronous function to scrape multiple URLs concurrently\n",
    "async def scrap(urls):\n",
    "    # Initialize an empty list to hold tasks\n",
    "    tasks = []\n",
    "    \n",
    "    # Create an aiohttp ClientSession for making HTTP requests\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Iterate over the provided list of URLs\n",
    "        for url in urls:\n",
    "            # Create a fetch task for each URL and append it to the tasks list\n",
    "            tasks.append(fetch(session, url))\n",
    "        \n",
    "        # Use asyncio.gather to run all fetch tasks concurrently\n",
    "        await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of URLs to be scraped\n",
    "urls = ['https://open.gsa.gov/', 'https://www.python.org/', 'https://www.google.com/']\n",
    "\n",
    "# Run the scrap function with the list of URLs using asyncio's run method\n",
    "# This will initiate the asynchronous scraping process for all specified URLs\n",
    "asyncio.run(scrap(urls=urls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
